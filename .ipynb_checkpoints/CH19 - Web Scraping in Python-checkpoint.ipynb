{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last chapter of the book covers the concept of web scraping. This is the programmatic process of obtaining information from a web page. To do this we need to get up to speed on a number of things:\n",
    "\n",
    "- HTML\n",
    "- Obtaining a webpage\n",
    "- Getting information from the webpage\n",
    "\n",
    "To do this we will create our own website using Python that we will scrape with our own code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Introduction to HTML\n",
    "- HTML stands for **Hyper Text Markup Language** and is the standard **markup language** for creating web pages. \n",
    "- It is essentially the language that makes up what you see on the internet.\n",
    "- An HTML file tells a web browser how to display the text, images, and other content on a webpage. \n",
    "- The purpose of HTML is to describe **how the content is structured** and **not how it will be styled**, and rendered within a web browser. \n",
    "- To render the page you need to use a cascading style sheet (CSS) and an HTML page can link to a CSS file to get information on colours, fonts, and other information relating to the rendering of the page.\n",
    "- HTML is a markup language, so in creating HTML content, you are embedding the text to be displayed alongside how the text should be displayed. \n",
    "- The way this is done is by using **HTML tags** which can contain **name-value pairs** which are known as **attributes**. \n",
    "- Information within a tag is known as an HTML element. \n",
    "- Well-formed HTML should have an open and a close tags, and before you start a new tag you should close off your old tag.\n",
    "- Now that we have described what HTML is, we will give some examples of how you create elements within it and show how to put together a page. \n",
    "- Let’s start by looking at some tags. \n",
    "- It is important to remember that when we open a tag, we close it with a / (forward slash).\n",
    "- Let’s demonstrate with a header tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Header"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is how we define a header:\n",
    "\n",
    "<h>This is how we define a header</h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h>This is how we define a header</h>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, we see that to open the tag we have\n",
    "\n",
    "<h1>\n",
    "\n",
    "and to close it we have\n",
    "\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "\n",
    "and to close it we have\n",
    "\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paragraph"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we show how to tag a paragraph:\n",
    "\n",
    "<p>This is how we define a paragraph</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This is how we define a paragraph</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, we show how to define a tag, which is how we have embedded a hyperlink:\n",
    "\n",
    "<a href=\"https://www.google.com\">This is how we define a link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.google.com\">This is how we define a link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The next HTML tag is for a table which is a bit more complex than what we have covered before.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Name</th>\n",
    "        <th>Year</th>\n",
    "        <th>Month</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Avengers: Infinity War</td>\n",
    "        <td>2018</td>\n",
    "        <td>March</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Ant Man and the Wasp</td>\n",
    "        <td>2018</td>\n",
    "        <td>August</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Name</th>\n",
    "        <th>Year</th>\n",
    "        <th>Month</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Avengers: Infinity War</td>\n",
    "        <td>2018</td>\n",
    "        <td>March</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Ant Man and the Wasp</td>\n",
    "        <td>2018</td>\n",
    "        <td>August</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So, the first thing we need to define for a table is the table tag:\n",
    "\n",
    "<table></table>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we need to define the rows in the table using the table row tag. This is denoted using:\n",
    "\n",
    "<tr></tr>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, we have three rows defined. In each row we need to have some data so you’ll see we use:\n",
    "\n",
    "<th> and <td>\n",
    "\n",
    "The <th> tags refer to the table header and <td> the table data. So, here we have the headers being Name, Year, and Month and then the next two rows are the table data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thead and Tbody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are two other tags that we can add to this table and that is the **thead** and **tbody** tag.\n",
    "- Within a table these can separate out the head and body of the table. \n",
    "- They are used as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "    <th>Name</th>\n",
    "    <th>Year</th>\n",
    "    <th>Month</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "    <tr>\n",
    "        <td>Avengers: Infinity War</td>\n",
    "        <td>2018</td>\n",
    "        <td>March</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Ant Man and the Wasp</td>\n",
    "        <td>2018</td>\n",
    "        <td>August</td>\n",
    "    </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "    <th>Name</th>\n",
    "    <th>Year</th>\n",
    "    <th>Month</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "    <tr>\n",
    "        <td>Avengers: Infinity War</td>\n",
    "        <td>2018</td>\n",
    "        <td>March</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Ant Man and the Wasp</td>\n",
    "        <td>2018</td>\n",
    "        <td>August</td>\n",
    "    </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The last tag we will introduce is a div tag. \n",
    "- This is a tag that defines a **section in the html**.\n",
    "- So, linking back to the previous table example we can put a div tag around it. \n",
    "- By putting html within a div, we can apply the format to the whole section covered by it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div>\n",
    "<table>\n",
    "<thead>\n",
    "    <th>Name</th>\n",
    "    <th>Year</th>\n",
    "    <th>Month</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "    <tr>\n",
    "        <td>Avengers: Infinity War</td>\n",
    "        <td>2018</td>\n",
    "        <td>March</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Ant Man and the Wasp</td>\n",
    "        <td>2018</td>\n",
    "        <td>August</td>\n",
    "    </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<table>\n",
    "<thead>\n",
    "    <th>Name</th>\n",
    "    <th>Year</th>\n",
    "    <th>Month</th>\n",
    "</thead>\n",
    "<tbody>\n",
    "    <tr>\n",
    "        <td>Avengers: Infinity War</td>\n",
    "        <td>2018</td>\n",
    "        <td>March</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Ant Man and the Wasp</td>\n",
    "        <td>2018</td>\n",
    "        <td>August</td>\n",
    "    </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML Attributes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Having defined lots of the tags, we will now discuss the attributes of these tags. \n",
    "- Attributes provide us with additional information about the elements. \n",
    "- We will show how these relate back to the tags we defined earlier. \n",
    "- Let’s begin by showing an example of an attribute applied to an <a> tag."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<a href='https://www.google.com\">This is how we define a link</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Here, the href is the attribute and it specifies what the url is.\n",
    "- We could also add a title to a tag which would result in the value being displayed as a tooltip (when you hover over it)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<p title=\"It will show when you hover over the text\">This is how we define a paragraph</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p title=\"It will show when you hover over the text\">This is how we define a paragraph</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Id and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Having introduced attributes, we will now look at two important ones which can help us **locate elements within html**, these are the **id** and **class** **attributes**. \n",
    "- An id element is unique to that html element whereas a class can be used in multiple elements. \n",
    "- Let’s demonstrate this by looking at three headers with associated information."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!-- A unique element -->\n",
    "<h1 id='myHeader'>MCU Films</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- A unique element -->\n",
    "<h1 id='myHeader'>MCU Films</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!-- Multiple similar elements -->\n",
    "<h2 class='film'>Avengers: Infinity War</h2>\n",
    "<p>Can Earth's mightiest heroes protect us from the threat of Thanos?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Multiple similar elements -->\n",
    "<h2 class='film'>Avengers: Infinity War</h2>\n",
    "<p>Can Earth's mightiest heroes protect us from the threat of Thanos?</p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<h2 class='film'>Ant Man and the Wasp</h2>\n",
    "<p>Following the events of Civil War will Scott Lang don the Ant Man suit again?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class='film'>Ant Man and the Wasp</h2>\n",
    "<p>Following the events of Civil War will Scott Lang don the Ant Man suit again?</p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<h2 class='film'>Captain Marvel</h2>\n",
    "<p title=\"I don't know\">Plot Unknown.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class='film'>Captain Marvel</h2>\n",
    "<p title=\"I don't know\">Plot Unknown.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As per below, we have one header with an id attribute. \n",
    "- This unique attribute specifically identifies that header. \n",
    "- The remaining headers all have the same class film which has been applied to each one. \n",
    "- This is only intended as a brief introduction to html, so while this will help us in the remainder of the chapter, it is not a comprehensive exploration of all things html.\n",
    "- So, if you are interested, there are lots of resources online which cover html."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!-- A unique element -->\n",
    "<h1 id='myHeader'>MCU Films</h1>\n",
    "\n",
    "<!-- Multiple similar elements -->\n",
    "<h2 class='film'>Avengers: Infinity War</h2>\n",
    "<p>Can Earth's mightiest heroes protect us from the threat of Thanos?</p>\n",
    "\n",
    "<h2 class='film'>Ant Man and the Wasp</h2>\n",
    "<p>Following the events of Civil War will Scott Lang don the Ant Man suit again?</p>\n",
    "\n",
    "<h2 class='film\">Captain Marvel</h2>\n",
    "<p>Plot Unknown.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Having introduced html and how it works, we now move onto how we obtain the data using Python. \n",
    "- When we think of web scraping, we generally think of the process of getting and processing data from a website. \n",
    "- Actually this can be broken down into two distinct processes: **web crawling** and **web scraping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Web crawling** is the process of **getting data from one or more urls** which can be obtained by **traversing through a websites** html. For example, say a website has a front page with lots of links to other pages and you want to get all the information from all the links, you would traverse through all the links programmatically and then visit all the relevant pages and store them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Web scraping** is the process of **getting the information** from the page in question, so in the previous part you would have to scrape the page to get the links that you want to traverse. When you scrape the page, you would programmatically get the information from the page and when you have that you would be able to store or process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given scraping plays an important part in the whole process, the combination of crawling and scraping will be referred to as web scraping. \n",
    "- There is no code of conduct that you need to sign up in order to conduct web scraping activities. \n",
    "- However, as soon as you try to get data from a website there are some things to note that are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if you are allowed to get and use data from the respective website.** \n",
    "\n",
    "While you may think that any data on the website is fair game, it is not always the case. Check the website’s terms of use as while they may not be able to stop you obtaining the data via web scraping, they may have something to say that if they see the data used in any research, so be careful. The issues around legality of getting data from the web are really important and if you are in any doubt, please get legal advice. The examples used in this book will involve creating our own web page locally and getting the data from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if there is a fair usage policy.** \n",
    "\n",
    "Some websites are happy for you to scrape the data as long as you do it in an appropriate way. What do we mean by that? Well, every time you make a call to the website, you are providing traffic to that site. By doing this via a computer, you can send a lot of traffic to a site very quickly and this can be problematic to the site. If you are seen to do this, your IP address can be blocked from the site which would mean you wouldn’t be able to access the site in question. So, what you need to consider is how often you plan to run code to hit certain websites and what is appropriate and necessary for you and whether the site will allow you to do. For code that does a call to a single url, it is just about how often you run it. However, if you wrote a code that crawled across lots of urls and brought back the data from them, then you would need to ensure that your code is running at an appropriate speed. To do this you would need to consider adding time delays to what you do to ensure that you are not sending excessive traffic to the site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Robots.txt** \n",
    "\n",
    "Again, linked to the above points, if you go to the **websites url/robots.txt**, you will get information on what web crawlers can and can’t do. If present, then it’s expected that you read and understand what can and can’t be scrapped. If there is no robots.txt, then there is no specific instruction on how the site can be crawled. However, don’t assume you can scrape everything on the site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ultimately, you need to take care when scraping a website and if they have an application programming interface (API) available, then you should be using that. \n",
    "- If you are not sure, please get the appropriate advice. \n",
    "- Before you can start crawling the site and scraping the data you need to understand the page. \n",
    "- Python cannot just get you the data you want, instead you need to tell it how to find the data you are after. \n",
    "- So you need to understand how the html works and where to look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the page you have a couple of options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Through a Web Browser**\n",
    "\n",
    "- You can use the tools of the web browser to inspect what HTML refers to what elements of the page. \n",
    "- The manner in which you can inspect is specific to the browser itself. \n",
    "- Ultimately, it involves you selecting the element of the page and inspecting the corresponding HTML and then it showing what the html refers to that element. \n",
    "- Different browsers have different ways of inspecting the pages they show, so refer to the documentation around the specific browser that you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the Page and Physically Searching**\n",
    "\n",
    "- You can physically save the page and then search for the name or value of certain text and in the same way determine what html refers to that element. \n",
    "- Ultimately, what you are trying to do is learn what html refers to what values in the website. \n",
    "- This isn’t an exact science due to the fact html can be written in different ways. \n",
    "- The key is understanding the definitions of html and how they fit together and then use this to understand what you need to access in the html. \n",
    "- With any piece of code you need to plan ahead and with parsing html you need to develop a plan for how you want to get the data from the html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, going back to what we mentioned at the start, we described web scraping and web crawling. \n",
    "- Web crawling is the process of accessing the data from a url or multiple urls. \n",
    "- To do this, we need a mechanism, luckily making a web request can be in the same way we accessed an API endpoint in the Chapter 18, so we will use the requests library. \n",
    "- This will be demonstrated later in the chapter where we setup our own webpage.\n",
    "- Having a mechanism to get the data is great but we also need to process what we get back so we need a Python library that can do this. \n",
    "- Python has many options and this book isn’t intended to be a review of the best packages for processing html as the landscape is constantly changing. \n",
    "- Instead we will cover one specific parser namely BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BeautifulSoup is not only an **html parser** but can also **parse xml by using the lxml library** which we covered earlier in the book. \n",
    "- The way that BeautifulSoup works, is that it takes advantage of other parsers. \n",
    "- So, to run BeautifulSoup you would:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup(content_to_be_parsed, \"parser_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **content_to_be_parsed** is the content from the site which could have been **obtained using requests** as shown before and the ‘parser name’ is the name of the parser to use. The four examples of these parsers are:\n",
    "\n",
    "- **html.parser**: This is the default Python html parser. It has decent speed and is lenient in how it accepts html as of 3.2.2.\n",
    "- **lxml**: This is built upon the lxml Python library which is built upon the C version. It’s very fast and very lenient.\n",
    "- **lxml-xml or xml**: Again built upon lxml so similar to above. However, this is the only xml parser you can use with BeautifulSoup. So, while we introduced how to parse XML with lxml, you could also do the same in BeautifulSoup.\n",
    "- **htmllib5**: This is built upon the Python html5lib and is very slow, however, it’s very lenient and it parses the page in the same way a web browser does to create valid html5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the rest of this section, we will concentrate on using the **html.parser**. So to create soup we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Film.py\n",
    "\n",
    "url = \"http://127.0.0.1:5000/films\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response_text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, this have transformed the html into a format where we can access elements within it. \n",
    "- What we will show now are the methods available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example:\n",
    "\n",
    "text = \"<b class='boldest'>This is bold</b>\"\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access this tag as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.b\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we had **multiple b tags** using soup.b would only **return the first one**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"<b class='boldest'>This is bold</b><b class='boldest'>This is also bold</b><h4 class='film'>This is header</h4>\"\"\"\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.b\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_h4 = soup.h4\n",
    "tag_h4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we won’t get all the b tags, back only the first one. The tag itself has a name which can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_h4.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag also has a dictionary of attributes which are accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_h4.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_h4['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s have a look in a more complicated example. If we look at something like a table we can parse it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Name</th>\n",
    "        <th>Year</th>\n",
    "        <th>Month</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Avengers: Infinity War</td>\n",
    "        <td>2018</td>\n",
    "        <td>March</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Ant Man and the Wasp</td>\n",
    "        <td>2018</td>\n",
    "        <td>AUgust</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can access elements of the table by just traversing down the tree structure of the html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.table.tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.table.tr.th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, in each example we get the **first instance of the tag** that we are looking for. \n",
    "- Assume we want to **find all tr tags** within the table, we can do so as follows using the **find_all** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, here we get back a list of all the tr tags. \n",
    "- Similarly, we can get back the list of td tags using the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_tags = soup.find_all('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with regards to get data out, if we look back and consider our original table, we can use the find all method to get all the tr tags and then loop over these and get the td tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows = soup.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = []\n",
    "content = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in table_rows:\n",
    "    header_tags = tr.find_all('th')\n",
    "    if len(header_tags) > 0:\n",
    "        for ht in header_tags:\n",
    "            header.append(ht.text)\n",
    "    else:\n",
    "        row = []\n",
    "        row_tags = tr.find_all('td')\n",
    "        for rt in row_tags:\n",
    "            row.append(rt.text)\n",
    "        content.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What we are doing here is looping over the high level tr tags to get every row and then looking for the th tags and if we find them, we know its the table header and if not we get the td tags and associate both with the appropriate list namely headers or content. \n",
    "- The important thing to note here is that we know the structure of the data as we will have inspected the html so we build the parsing solution knowing what we will get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, we have introduced the find_all method in a single table. \n",
    "- But if we had two tables and the table we wanted had a specific id we could use the find method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "<table id=\"unique_table\">\n",
    "<tr>\n",
    "<th>Name</th>\n",
    "<th>Year</th>\n",
    "<th>Month</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Avengers: Infinity War</td>\n",
    "<td>2018</td>\n",
    "<td>March</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Ant Man and the Wasp</td>\n",
    "<td>2018</td>\n",
    "<td>August</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table id=\"second_table\">\n",
    "<tr>\n",
    "<th>Name</th>\n",
    "<th>Year</th>\n",
    "<th>Month</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Avengers: End Game</td>\n",
    "<td>2019</td>\n",
    "<td>April</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Spider-man: Far from home</td>\n",
    "<td>2019</td>\n",
    "<td>June</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table id=\"other_table\">\n",
    "<tr>\n",
    "<th>Name</th>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can get the data from this table in a similar way as before but again we can take advantage of the find method to find the text in the specific element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2 = soup.find('table', id='second_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows = table_2.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = []\n",
    "content = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in table_rows:\n",
    "    header_tags = tr.find_all('th')\n",
    "    if len(header_tags) > 0:\n",
    "        for ht in header_tags:\n",
    "            header.append(ht.text)\n",
    "    else:\n",
    "        row = []\n",
    "        row_tags = tr.find_all('td')\n",
    "        for rt in row_tags:\n",
    "            row.append(rt.text)\n",
    "        content.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This shows that, when we have multiple tables, we can obtain the information from a specific one, this is really dependent on the table having an id attribute which made the process much easier.\n",
    "- So, we now know how to process html, and the next stage is to grab data from a website and then parse that data using Python. - To do this we will build our own website locally which we will grab data from and parse the results. \n",
    "- Given we have covered the libraries to get and process the data how do we go about creating a website?\n",
    "- As in the Chapter 18, we will use the package Flask to create a simple website that we can run locally and then scrape the data from. \n",
    "- Let’s just get started and write a hello world example to show how it will work. \n",
    "- Here, we will create a file called **my_flask_website.py** and put the following code in it:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def helloworld():\n",
    "    return 'Hello World'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, if you think back to the Chapter 18, what we have here is a reduced down version of what we used to create our API. \n",
    "- We import Flask from the flask package and then create ourselves an app. \n",
    "- Unlike with the API where we created a class, we simply define a **hello_world function** which returns the string Hello World."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@app.route('/')\n",
    "def helloworld():\n",
    "    return 'Hello World'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use the syntax below to run our application."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As with the API we built, if we open a terminal or command prompt and move to the location of the file and run the code using **python my_flask_website.py**, then we will get a webpage running on http://127.0.0.1:5000/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, one part of the code that we didn’t cover was the use of @app.route, this is an example of a decorator. The purpose is to bind a location to a function.\n",
    "- So, when we apply the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@app.route('/')\n",
    "def helloworld():\n",
    "    return 'Hello World'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What we are doing is mapping any call of http://127.0.0.1:5000/ to the **function hello_world**, so when that url is called the hello_world function is executed and the results displayed. \n",
    "- This is a specific use of a decorator where in general, decorators are functions that can take functions as an argument. \n",
    "- The best way to explain is by demonstration so we could decorate the hello_world function with a decorator that make a string all lowercase."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def make_uppercase(function):\n",
    "    def wrapper():\n",
    "        func = function()\n",
    "        lowercase = func.lower()\n",
    "        return lowercase\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What this function does is take in another function as an argument and then run the wrapper function in the return statement.\n",
    "- The function wrapper then runs the function that is passed in to make_uppercase function and take the output from it and make it lowercase and return that value.\n",
    "-  Let’s demonstrate with an example."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# HelloWorldLowercase.py\n",
    "\n",
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "def make_lowercase(function):\n",
    "    def wrapper():\n",
    "        func = function()\n",
    "        lowercase = func.lower()\n",
    "        return lowercase\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "@app.route('/')\n",
    "@make_lowercase\n",
    "def hello_world():\n",
    "    return 'Hello World'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have our website up and running, we can see the Hello World is displayed in lowercase.\n",
    "- So, let’s programmatically get the data from it. \n",
    "- To do this, we can use requests in the same way we did in the API chapter to obtain a get request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://127.0.0.1:5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that this time we looked at the text attribute as opposed to the json method and that is because the content of our website is not json. \n",
    "- This is all well and good but its not much of a challenge to process the data mainly because its not in html format.\n",
    "- We can change that pretty easily by just modifying the code in our flask application so let’s change the output hello world as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# HelloWorldHTMLHeading.py\n",
    "\n",
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return '<h>Hello World</h>'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks pretty similar to what we saw before, so what has changed? \n",
    "- If we run the code to get the data from the webpage, we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://127.0.0.1:5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can now see that instead of just the text representation, we have some html around that with the h tags.\n",
    "- Let’s modify the code once more and change the h tags to h1 tags."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# HelloWorldHTMLHeading1.py\n",
    "\n",
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return '<h1>Hello World<h1>'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again running the same requests code on this website bring back the h1 tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://127.0.0.1:5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, now we have our website running, lets add something a little harder to parse and create a table that we can look to programmatically obtain. \n",
    "- To do this, we will add a new route to the flask application and look to add a html table. \n",
    "- To do this, we will make use of some existing data from the seaborn package namely the tips data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head().to_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we have imported the head of tips dataset and we can make use of the to_html method from pandas, which takes the DataFrame and give us back html that we could put on our website. \n",
    "- If we look back to our previous table example, we might want to add an id to the table to allow us to access the table and we can do that using to_html by passing in the table_id argument and setting it to the name that we want our table to have. \n",
    "- So, let’s apply it by setting the name to be tips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head().to_html(table_id='tips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, we can now see we have added the id attribute with the name tips. \n",
    "- Our next step is to add this to our website and we can alter the code as follows to do so:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tips.py\n",
    "\n",
    "from flask import Flask\n",
    "import seaborn as sns\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return '<h1>Hello World</h1>'\n",
    "\n",
    "@app.route('/table')\n",
    "def table_view():\n",
    "    return tips.head(20).to_html(table_id='tips')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, the difference here is that we have added the imports for seaborn and then imported the tips dataset. \n",
    "- To display this, we then create another function called **table_view** and in it return 20 rows of the DataFrame and convert it to html with the **id of tips**. \n",
    "- A **decorator** then defines the route of this to be **/table** which means when we go to the http://127.0.0.1:5000/table, we will see the result of this function. \n",
    "- Let’s do that and go to the url.\n",
    "- Now, we can see the table but it doesn’t look great, we can customise this using some of the options that come with pandas.\n",
    "- First, we will remove the index from the table as you normally wouldn’t see this on a website. \n",
    "- Next, we will centre the table headings and we will also make the borders more prominent. \n",
    "- So, our flask application is now modified to this."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tips_Table_Customization.py\n",
    "\n",
    "from flask import Flask\n",
    "import seaborn as sns\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return '<h1>Hello World</h1>'\n",
    "\n",
    "@app.route('/table')\n",
    "def table_view():\n",
    "    return tips.head(20).to_html(table_id='tips', border=6, index=False, justify='center')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we use some of what we covered earlier, we can add a title and some information about the website in a paragraph. \n",
    "- To do that, we can use the h1 and p tags to create a header and paragraph, respectively, and to show that everything belongs together let’s put this all within a div tag, so it resembles what you might find on a production web page. \n",
    "- The flask application now looks like the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tips_Table_Webpage.py\n",
    "\n",
    "from flask import Flask\n",
    "import seaborn as sns\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return '<h1>Hello World</h1>'\n",
    "\n",
    "@app.route('/table')\n",
    "def table_view():\n",
    "    html = '<div>' + \\\n",
    "           '<h1>Table of tips data</h1>' + \\\n",
    "           '<p>This table contains data from the seaborn tips dataset</p>' + \\\n",
    "           tips.head(20).to_html(table_id='tips', border=6, index=False, justify='center') + \\\n",
    "           '</div>'\n",
    "    return html\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have a website. We want to scrape it, so let’s use requests to get the html that we will look to obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://127.0.0.1:5000/table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, we can see that it was relatively straight forward to get the data but unlike with our static table example before, the data from the webpage is more than just table data. \n",
    "- The next step is to pass this into BeautifulSoup to parse the html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('table', id='tips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By using the table id, we can go directly to the table within the html and we then have access to all the rows within it just like before. \n",
    "- Note that we have only shown a subset of this data as we have 20 rows. \n",
    "- Now, if we want to parse the data from the html we can use something like we used on the dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table', id='tips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows = table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = []\n",
    "content = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in table_rows:\n",
    "    header_tags = tr.find_all('th')\n",
    "    if len(header_tags) > 0:\n",
    "        for ht in header_tags:\n",
    "            header.append(ht.text)\n",
    "    else:\n",
    "        row = []\n",
    "        row_tags = tr.find_all()\n",
    "        for rt in row_tags:\n",
    "            row.append(rt.text)\n",
    "        content.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have now pulled the data from the html and got it into two separate lists, bit to go a step further we can put it back into a DataFrame pretty simply by using what we have covered earlier in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we have gone full circle and used a DataFrame to populate a table within our website and then scraped that data and converted it back into a DataFrame.\n",
    "- This chapter has covered a lot of content from introducing html to parsing it out to building our own website and scraping from there. \n",
    "- The examples have been focussed on table data but they can be applied to any data we find within html. \n",
    "- When it comes to web scraping, Python is a powerful and popular choice to interact and obtain data from the web."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
