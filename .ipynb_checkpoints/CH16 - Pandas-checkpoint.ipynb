{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is a package that is used for data analysis and data manipulation. \n",
    "# It’s used in a variety of packages and therefore understanding of it and its concepts is a crucial tool \n",
    "# for a Python programmer to learn. \n",
    "# In this chapter, we will introduce the pandas package from the basics up to some more advanced techniques. \n",
    "# However, before we get started with pandas, we will briefly cover numpy arrays which alongside dictionaries \n",
    "# and lists are concepts that should be understood to allow us to cover pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Arrays\n",
    "\n",
    "# Numpy comes as part of the Anaconda distribution and is a key component in the scientific libraries within Python. \n",
    "# It is very fast and underpins many other packages within Python.\n",
    "# We concentrate on one specific aspect of it, numpy arrays. \n",
    "# However, if you are interested in any of the machine learning libraries within Python, then numpy is certainly something\n",
    "# worth exploring further.\n",
    "# We can import it as follows: \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why np? Its the standard convention used in the documentation.\n",
    "# However you do not have to use that convention but we will. \n",
    "# In this chapter, we won’t cover everything to do with numpy but instead only introduce a few concepts \n",
    "# and the first one we will do is introduce an array. \n",
    "# An array in numpy is much like a list in Python. \n",
    "# If we want to create an array of integers 0–10 we can do so as follows:\n",
    "\n",
    "number_array = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "number_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like we passed a list into the method array and that is basically what we did as we can define the same array.\n",
    "\n",
    "number_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "number_array = np.array(number_list)\n",
    "number_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may be thinking it looks like a list and we can use a list to create it, why is it different from a list. \n",
    "# Very early on we looked at lists and operations on lists and we saw that using the common mathematical operators \n",
    "# either didn’t work or worked in an unexpected way.\n",
    "# We will now cover these again and compare them to what happens when using an array in numpy. \n",
    "# We will begin by looking at addition:\n",
    "\n",
    "number_list + number_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_array + number_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, what we see is that with a list we have concatenation of two lists which is what we have seen before.\n",
    "# However using an array we add together the two arrays and return a single array where the result is the \n",
    "# element wise addition. \n",
    "# Next, let’s consider what happens when we use the mathematical subtraction symbol.\n",
    "\n",
    "number_list - number_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_list * number_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_array * number_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we see that this operand doesn’t work on two lists but the arrays provide elementwise multiplication. \n",
    "# Now for completion we will look at the division operand on both lists and arrays.\n",
    "\n",
    "number_list / number_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_array / number_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsurprisingly, we see that this doesn’t work on lists but on the arrays it performs elementwise division of the values \n",
    "# in the first array by those in the second. \n",
    "# This is great if we want to perform some mathematical operation on two lists which we cannot do and can be much faster. \n",
    "# For the examples we have covered so far we can achieve the same thing using lists in Python in one line via \n",
    "# list comprehension. \n",
    "# So the three examples that didn’t work here can be rewritten in as follows:\n",
    "\n",
    "number_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtraction_list = [n - n for n in number_list]\n",
    "subtraction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplication_list = [n * n for n in number_list]\n",
    "multiplication_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "division_list = [n / n for n in number_list]\n",
    "division_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, it should be noted that we have simply worked with operations on the same list which makes it easy to rewrite.\n",
    "# However if we had two distinct lists we cannot use list comprehension and to rewrite using loops becomes more difficult. \n",
    "# Let’s demonstrate this by creating two random integer arrays.\n",
    "# So in numpy, we can do this by using the random choice functionality as follows:\n",
    "\n",
    "np.random.choice(10,10)\n",
    "\n",
    "# Here, we generated an array of length 10 containing random numbers between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we extend this example to 1 million random numbers and generate two arrays we can multiply them together as follows:\n",
    "\n",
    "x = np.random.choice(100,1000000)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.choice(100,1000000)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = x*y\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we have just done is complete 1 million multiplications instantly.\n",
    "# If you try this using loops you would be waiting quite a bit longer than an instance! \n",
    "# We have seen how powerful numpy arrays are but how do we access elements of them. \n",
    "# Luckily we can access elements as we did for lists. \n",
    "# We will give some examples below applied to the result array from the previous example:\n",
    "\n",
    "result[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see that we access elements in much the same we did for lists.\n",
    "# Now having introduced the concept of an array alongside everything else \n",
    "# means we can start looking at Pandas starting with Series.\n",
    "\n",
    "# Series\n",
    "\n",
    "# We can import pandas as follows:\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like before with numpy we use the alias pd which is the general convention used in the documentation for the package.\n",
    "# The first thing that we will cover here is the concept of a Series, we shall demonstrate this first by an example:\n",
    "\n",
    "point_dict = {\"Bulgaria\":45, \"Romania\":43, \"Hungary\":30, \"Denmark\":42}\n",
    "point_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_series = pd.Series(point_dict)\n",
    "point_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We created a dictionary containing the keys of country names and the median age of citizens (source worldomometers.info) \n",
    "# in that country and passes then in to the Series method to create point series. \n",
    "# We can access the elements of the series as follows:\n",
    "\n",
    "point_series[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_series[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_series[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_series[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_series[[1,3]]\n",
    "\n",
    "# point_series[1,3] will generate error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see we can access the first element as if it was a list using the position of the value we want. \n",
    "# We can also use the colon separated positional values as well as negative indices which we have covered earlier. \n",
    "# There is a different way we can access elements of the series and that is by passing a list of the positions we \n",
    "# want from the series. \n",
    "# So if we want the first and third elements we have the values 0 and 2 in the list.\n",
    "# We have just accessed the values of the dict that we passed in to create the series but what about the keys and \n",
    "# what use do they have in the series? \n",
    "# What we will now show is that the series can also be accessed like it was a dictionary:\n",
    "\n",
    "point_series.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_series[\"Bulgaria\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see the series has an index which is the key of the dictionary and we can access the values using the \n",
    "# dictionary access approach we have seen earlier. \n",
    "# Now, when we covered dictionaries earlier we saw that we could try and access a value from a key that isn’t \n",
    "# in the dictionary and it would throw an exception which is the same for the series.\n",
    "\n",
    "point_series[\"England\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we have shown what happens but you can see we have used the method 'get' to try and access the value \n",
    "# for the index England. \n",
    "# As opposed to throwing an exception it just returns None.\n",
    "\n",
    "point_series.get(\"England\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given we can now access elements of a series we will now show how you can operate on it. \n",
    "# Given the series is based on the concept of an array in numpy you can do much of what you would in numpy to the series. \n",
    "# So, now we will create series of random numbers and show how we can operate on them.\n",
    "\n",
    "np.random.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we have used numpy’s random methods to generate an array of 10 random numbers between 0 and 1. \n",
    "# This can be assigned to a series relatively easily.\n",
    "\n",
    "random_series = pd.Series(np.random.rand(10))\n",
    "random_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can operate on this in much the same way as we do with a numpy array.\n",
    "\n",
    "random_series_one = pd.Series(np.random.rand(10))\n",
    "random_series_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_series_two = pd.Series(np.random.rand(10))\n",
    "random_series_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_series_one + random_series_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_series_one / random_series_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that all looks the same as we have seen for arrays earlier.\n",
    "# However one key difference is that we can operate on splices of the series.\n",
    "\n",
    "random_series_one[3:] * random_series_two[:-1]\n",
    "\n",
    "# What we see is that the multiplication is done on the elements of the series by index.\n",
    "# So where we don’t have an index for both series we get an NaN shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We earlier defined a series by using a dictionary but we can define a series using a list or array as follows:\n",
    "\n",
    "pd.Series([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array([1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see the index is defined automatically by pandas.\n",
    "# However if we want a specific index we can define one as follows:\n",
    "\n",
    "pd.Series([1,2,3,4,5], index=['a','b','c','d','e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1,2,3,4,5], index=[5,4,3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, here we pass an optional list to the index variable and this gets defined as the index for the series. \n",
    "# It must be noted that the length of the index list must match that of the list or array that we want to make a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames\n",
    "\n",
    "# Having looked at series, we will now turn our attention to data frames which are arguably the most popular aspect \n",
    "# of pandas and are certainly what we use the most. \n",
    "# They are essentially an object that carries data in column and row format, so for many they will mimic what is held \n",
    "# in a spreadsheet or for others the content of a database table.\n",
    "# We will start off by looking at how we create a DataFrame and like with a series there are many ways we can do it.\n",
    "\n",
    "countries = [\"United Kingdom\",\"France\",\"Germany\",\"Spain\",\"Italy\"]\n",
    "median_age = [40,42,46,45,47]\n",
    "country_dict = {\"name\":countries, \"median age\":median_age}\n",
    "country_df = pd.DataFrame(country_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we have done above is begin by setting up two lists, one containing names and another containing values. \n",
    "# These are then put into a dictionary with keys name and value.\n",
    "# This dictionary is then passed into the 'DataFrame' method of pandas and what we get back is a DataFrame object \n",
    "# with column names of name and values. \n",
    "# We can see here that the index is automatically defined as 0–4 to correspond with the number of elements in each list.\n",
    "\n",
    "countries = pd.Series([\"United Kingdom\",\"France\",\"Germany\",\"Spain\",\"Italy\"])\n",
    "median_age = pd.Series([40,42,46,45,47])\n",
    "\n",
    "country_dict = {\"name\":countries, \"median age\":median_age}\n",
    "country_df = pd.DataFrame(country_dict)\n",
    "country_df\n",
    "\n",
    "# We can do the same using a dictionary of Series again assigning the Series to a dictionary and passing it into \n",
    "# the DataFrame method. \n",
    "# The same would happen if we used numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we create a DataFrame using a list of tuples where the data is now country name, median age and density of the country.\n",
    "\n",
    "data = [(\"United Kingdom\",40,281),(\"France\",42,119),(\"Italy\",46,206)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we have created a list of tuples and we then pass those into the 'DataFrame' method and it returns a three column \n",
    "# by three row data frame. \n",
    "# Unlike before we not only have auto assigned index values but we also have auto assigned column names which aren’t the \n",
    "# most useful.\n",
    "# However we will later show how to assign both. \n",
    "# The same applies here for a list of lists, list of series, or a list or arrays. \n",
    "# It also works for a list of dictionaries, however the behaviour is slightly different.\n",
    "\n",
    "data = [{\"country\":\"United Kingdom\", \"median age\":40, \"density\":281}, \n",
    "        {\"country\":\"France\", \"median age\":42, \"density\":119}, \n",
    "        {\"country\":\"Italy\", \"median age\":46, \"density\":206}]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the list of dictionaries is passed in we get the same DataFrame.\n",
    "# However now we have column names from the dictionary. \n",
    "# On the face of it everything seems like it works the same as for lists of lists. \n",
    "# However if we change some of the keys we get some different behaviour.\n",
    "\n",
    "data = [{\"country\":\"United Kingdom\", \"median age\":40, \"density\":281},\n",
    "       {\"country\":\"France\", \"median age\":42, \"density\":119},\n",
    "       {\"country\":\"Italy\", \"median\":46, \"density\":206}]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we see here is that as every dictionary doesn’t have all the same keys pandas fills in the missing values with NaN. \n",
    "# Next, we will look at how to access elements of the dataframe.\n",
    "\n",
    "data = [{\"country\":\"United Kingdom\", \"median age\":40, \"density\":281},\n",
    "       {\"country\":\"France\", \"median age\":42, \"density\":119},\n",
    "       {\"country\":\"Italy\", \"median age\":46, \"density\":206}]\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"country\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"country\"][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first thing is that we have defined the data frame based on the list of dictionaries as we showed previously. \n",
    "# We then accessed all the elements of the column country by passing the name of the column as the key of the data frame. \n",
    "# We next showed how we could access the first element of that by adding the index of the value we wanted. \n",
    "# This is an important distinction as we aren’t asking for the first element, we are instead asking for the value of the \n",
    "# column with index value 0. \n",
    "# Lastly, we select the rows of the country with index 0 and 1 in the usual way we would for a list, but again we are asking\n",
    "# for specific index rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"country\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we asked for the last value of the column country as we would with a list.\n",
    "# However it threw an error because there is no index −1 in the index for the data frame. \n",
    "# So we can’t treat the data frame as we would a list and we need to have an understanding of the index.\n",
    "# For any dataframe we can find out the index and columns as follows:\n",
    "\n",
    "data_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, it shows the index starting at 0 and stopping at 3 with the step used each time. \n",
    "# It also shows the columns as a list of each name. \n",
    "# We can change the index of a data frame as follows:\n",
    "\n",
    "data_df.index = ['a','b','c']\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we want to access the first element of the country column we do so as follows:\n",
    "\n",
    "data_df[\"country\"]['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly if we want to change the column names of a data frame we do so as follows:\n",
    "\n",
    "data_df.columns = [\"country_name\", \"median_age\", \"density\"]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given we have changed the index to strings, the question is how do we access the nth row if we don’t know what the index is. \n",
    "# Luckily there is a method of data frames called iloc which allow us to access the nth row by just passing in the number of \n",
    "# the row that we want. It works as follows:\n",
    "\n",
    "data_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see we can access rows from the data frame as if it was a list, which is cool.\n",
    "# Let’s say we want to add a column of all ones to our DataFrames we can do so as follows:\n",
    "\n",
    "data_df[\"ones\"] = 1\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then delete a column in a couple of ways:\n",
    "\n",
    "del data_df[\"ones\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"ones\"] = 1\n",
    "data_df.pop(\"ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we first used the 'del' method to delete the ones column, we then added it again and then used the 'pop' method \n",
    "# to remove the column. \n",
    "# Note that when we use the 'del' method, we simply delete from the DataFrame but using the 'pop' method we return the column we\n",
    "# have popped as well as removing it from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"ones\"] = 1\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"new_ones\"] = data_df[\"ones\"][1:2]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_df[\"new_ones\"]\n",
    "del data_df[\"ones\"]\n",
    "\n",
    "# What we see here is that when we use a partial column to form a new one, pandas knows to fill in the gaps with the NaN value. \n",
    "# There is another approach where we can insert a column and put it in a specific position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.insert(1,\"twos\",2)\n",
    "data_df\n",
    "\n",
    "# Here, we create a column containing the integer value 2 and puts it into position 1 \n",
    "# (remember position 0 is the first position) under the title twos. \n",
    "# This gives us full control of how we add to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_df[\"twos\"]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, now we have a grasp of what a DataFrame is we can start doing some cool things to it. \n",
    "# Let’s say we want to take all data where the value is less than 20.\n",
    "\n",
    "data_df[\"density\"] < 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[data_df[\"density\"] < 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we have done here is test the values in data_df values column to see which ones are less than 20. \n",
    "# This concept is a pretty key, we test every element in the column to see which ones are less than 20 and return \n",
    "# a boolean column to show which ones meet the criteria.\n",
    "# We can then pass this into the square brackets around a DataFrame and it returns the values where the condition is true. \n",
    "# We can do this on multiple boolean statements where anything true across all the statements is returned, this is shown below:\n",
    "\n",
    "data_df[data_df[\"density\"] < 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"median_age\"] > 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[(data_df[\"density\"] < 250) & (data_df[\"median_age\"] > 42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important to note that the DataFrame isn’t changed in this instance it stays the same.\n",
    "# To use the DataFrame that is returned from such an operation you need to assign it to a variable to use later.\n",
    "\n",
    "(data_df[\"density\"] < 250) & (data_df[\"median_age\"] > 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"test\"] = (data_df[\"density\"] < 250) & (data_df[\"median_age\"] > 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_df[\"test\"]\n",
    "\n",
    "# Here, we have used the same test as used in the previous example and assigned it to a column which is \n",
    "# now part of the DataFrame. \n",
    "# We could do the same thing if we wanted to create another column that uses the data in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"density\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"density_proportion\"] = data_df[\"density\"] / data_df[\"density\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we have divided all values in the value column with the sum of all the values in the column \n",
    "# which we can see is 606 to give us a new column of data. \n",
    "# We can also perform standard mathematical operations to a column. \n",
    "# Below we use the numpy exponential function to exponentiate every element of the column:\n",
    "\n",
    "np.exp(data_df[\"density\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also loop across a dataframe as we have seen before with lists.\n",
    "\n",
    "for n in data_df:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This isn’t exactly what we thought we would get as it only loops across the column names.\n",
    "# We really want to get into the meat of the dataframe to that we have to introduce the concept of transpose.\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we have done here is turn the DataFrame the other way so now the columns are the index. \n",
    "# To loop over it we use the 'iteritems' method.\n",
    "\n",
    "for n in data_df.T.iteritems():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we see here is that when we use the iteritems method over the data frame and at each instance \n",
    "# of the loop it returns a two element tuple. \n",
    "# The first element is the index and the second the values in the row stored in a series. \n",
    "# The better way to access it would be to assign each element to a variable allowing us to have better access to each part.\n",
    "\n",
    "for ind, row in data_df.T.iteritems():\n",
    "    print(ind)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in data_df.T.iteritems():\n",
    "    print(ind)\n",
    "    print(row[\"country_name\"])\n",
    "    \n",
    "# We assign the first element of the tuple to the variable ind and the series of the row in the variable row. \n",
    "# Then we access the country column of that row and show it here with the index. \n",
    "# Also we can avoid using the transpose of the dataframe by directly accessing the row via the 'iterrows' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in data_df.iterrows():\n",
    "    print(ind)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have looked at how to add columns to a data frame but now we will look at how to add rows. \n",
    "# The way we will consider is using the 'append' method on dataframes. \n",
    "# We do so as follows:\n",
    "\n",
    "data = [{\"country\":\"United Kingdom\", \"median_age\":40, \"density\":281},\n",
    "       {\"country\":\"France\", \"median_age\":42, \"density\":119},\n",
    "       {\"country\":\"Italy\", \"median_age\":46, \"density\":206}]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = [{\"country\":\"Iceland\", \"median_age\":37, \"density\":3}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_data_df = pd.DataFrame(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.append(new_row_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we setup the initial data as we have done earlier but here make a fresh copy of the original data. \n",
    "# We then setup a DataFrame of the new row and pass that into the 'append' method of the original dataframe. \n",
    "# What we then see is the DataFrame containing the new row.\n",
    "# However, it has an index of zero which we already had in the original DataFrame. \n",
    "# We also see that when we call the DataFrame after this operation it no longer has the new row.\n",
    "# If we look at the index problem we can resolve this by using the argument ignore_index as follows:\n",
    "\n",
    "data_df.append(new_row_data_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So that is sorted but what about the fact that the new row hasn’t become part of the data frame. \n",
    "# Well to get that to work we need to assign the data frame to a new variable as the append method doesn’t change \n",
    "# the original DataFrame. \n",
    "# We could re-assign the data frame to the same name data_df, however we would lose the memory of what we have done so\n",
    "# we could assign it to a new variable.\n",
    "\n",
    "new_data_df = data_df.append(new_row_data_df)\n",
    "new_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge, Join, and Concatenation\n",
    "\n",
    "# Initially, we will consider the concept of concatenating DataFrames. \n",
    "# The manner in which we can do this is to create a list of DataFrames and pass them into the 'concat' method.\n",
    "# These DataFrames will build on what we have looked at in the previous chapter by using the following country data:\n",
    "\n",
    "df1 = pd.DataFrame({\"density\":[119,206,240,94],\n",
    "                   \"median_age\":[42,47,46,45],\n",
    "                   \"population\":[65,60,83,46],\n",
    "                   \"population_change\":[0.22,-0.15,0.32,0.04]},\n",
    "                  index=[\"France\",\"Italy\",\"Germany\",\"Spain\"])\n",
    "\n",
    "df2 = pd.DataFrame({\"density\":[153, 464, 36, 25],\n",
    "                   \"median_age\":[38, 28, 38, 33],\n",
    "                   \"population\":[1439, 1380, 331, 212],\n",
    "                   \"population_change\":[0.39, 0.99, 0.59, 0.72]},\n",
    "                  index=[\"China\",\"India\",\"USA\",\"Brazil\"])\n",
    "\n",
    "df3 = pd.DataFrame({\"density\":[9, 66, 347, 103],\n",
    "                   \"median_age\":[40, 29, 48, 25],\n",
    "                   \"population\":[145, 128, 126, 102],\n",
    "                   \"population_change\":[0.04, 1.06, -0.30, 1.94]},\n",
    "                  index=[\"Russia\",\"Mexico\",\"Japan\",\"Egypt\"])\n",
    "\n",
    "frames = [df1,df2,df3]\n",
    "result = pd.concat(frames)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = pd.concat([df1,df2,df3])\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we did was to create a list of DataFrames and then by passing them into the pd.concat method. \n",
    "# We get the result shown which is DataFrame with columns density, median_age, population, population_change \n",
    "# and rows indexed with country names. \n",
    "# But what if we did not have the index values as shown in the example:\n",
    "\n",
    "df1 = pd.DataFrame({\"density\":[119, 206, 240, 94],\n",
    "                   \"median_age\":[42, 47, 46, 45],\n",
    "                   \"population\":[65, 60, 83, 46],\n",
    "                   \"population_change\":[0.22, -0.15, 0.32, 0.04],\n",
    "                   \"country_name\":['France', 'Italy', 'Germany', 'Spain']})\n",
    "\n",
    "df2 = pd.DataFrame({\"density\":[153, 464, 36, 25],\n",
    "                   \"median_age\":[38, 28, 38, 33],\n",
    "                   \"population\":[1439, 1380, 331, 212],\n",
    "                   \"population_change\":[0.39, 0.99, 0.59, 0.72],\n",
    "                   \"country_name\":['China', 'India', 'USA', 'Brazil']})\n",
    "\n",
    "df3 = pd.DataFrame({\"density\":[9, 66, 347, 103],\n",
    "                  \"median_age\":[40, 29, 48, 25],\n",
    "                  \"population\":[145, 128, 126, 102],\n",
    "                  \"population_change\":[0.04, 1.06, -0.30, 1.94],\n",
    "                  \"country_name\":['Russia', 'Mexico', 'Japan', 'Egypt']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1,df2,df3]\n",
    "result = pd.concat(frames)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we see that the index is retained for each DataFrame which when created all have the index 0, 1, 2, 3. \n",
    "# To have an index 0–11 we need to use the ignore_index argument and set it to True.\n",
    "\n",
    "result = pd.concat(frames, ignore_index=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can expand on this example by creating a list of DataFrames as we did previously and concat them together \n",
    "# but now we use the argument keys and set it to a list containing region one, region two, and region three.\n",
    "# However, you can't add-up ignore_index=True together as the argument keys will be ignored.\n",
    "\n",
    "result = pd.concat(frames, keys=[\"Region One\", \"Region Two\", \"Region Three\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[\"Region Two\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In running the code what we see is that passing the keys in means we have what appears to be \n",
    "# another level of the DataFrame away from our index in the previous example which allows us \n",
    "# to select the one of the DataFrames used in the concat. \n",
    "# If we look at the index of the result we get the following:\n",
    "\n",
    "result.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is commonly referred to a multilevel index as the name would suggest and what it does is tell us \n",
    "# what the index value each element has. \n",
    "# So the levels are [“region_one”, “region_two”, “region_three”] and [0, 1, 2, 3], which are denoted in levels. \n",
    "# The index for each row is then determined using the label which has two lists of eight elements with the first \n",
    "# one having values 0, 1, 2 which corresponds to region one, region two and region three whilst the second has \n",
    "# values 0, 1, 2, 3 which refer to the levels 0, 1, 2, 3. \n",
    "# We could name these levels by using the optional name argument.\n",
    "\n",
    "result = pd.concat(frames, keys=[\"Region_One\",\"Region_Two\",\"Region_Three\"], names=[\"Region\",\"Item\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the previous example we used concat to concatenate the DataFrames together.\n",
    "# However, there are other ways to use it which we will demonstrate now by concatenating urban \n",
    "# population percentage from France, Italy, Argentina, and Thailand to our initial DataFrame.\n",
    "\n",
    "df1 = pd.DataFrame({\"density\":[119, 206, 240, 94],\n",
    "                   \"median_age\":[42, 47, 46, 45],\n",
    "                   \"population\":[65, 60, 83, 46],\n",
    "                   \"population_change\":[0.22, -0.15, 0.32, 0.04]},\n",
    "                  index=['France', 'Italy', 'Germany', 'Spain'])\n",
    "\n",
    "df4 = pd.DataFrame({\"urban_population\":[82, 69, 93, 51]},\n",
    "                  index=['France', 'Italy', 'Argentina', 'Thailand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df4], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we have used concat with a list of DataFrames as we have done before but now we pass in the argument axis = 1. \n",
    "# Now, the axis argument says we concatenate on the columns, here 0 is index and 1 is columns. \n",
    "# So, we see commonality in the index with France and Italy, so we can add the extra column on and fill the values \n",
    "# that are not common with NaN. \n",
    "# Here, we have set the sort to be False which means we keep the order as if the two were joined one below the other. \n",
    "# If we set the value to be True we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df4], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that with sort set to True we get the values sorted by index order. \n",
    "# Below we can also see what happens if we run the same query with the axis set to 0.\n",
    "\n",
    "pd.concat([df1,df4], axis=0, sort=True)\n",
    "\n",
    "# What we do is just concatenate the DataFrames one below each other with duplication \n",
    "# of the index for France and Italy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat also has an extra argument join that we will now explore and set the value to join.\n",
    "\n",
    "pd.concat([df1,df4], axis=1, join=\"inner\")\n",
    "\n",
    "# As you can see we only have two rows returned which if you look back at the example\n",
    "# before are the only two rows where the two DataFrames have values in columns. \n",
    "# The inner join is similar to that of a database join.\n",
    "# However, here we don’t specify a key to use it on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we add argument join_axes and set it to df1.index.\n",
    "\n",
    "result = pd.concat([df1, df4], axis=1).reindex(df1.index)\n",
    "result\n",
    "\n",
    "# What we see is that all we get back only the values for in the index in df1 and show all\n",
    "# the columns from the axis 1 argument. \n",
    "# By default the join_axes are set to False.\n",
    "\n",
    "# Join_axes is deprecated. The supported way is now .reindex(df1.columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will ignore the index by using the following arguments:\n",
    "\n",
    "pd.concat([df1,df4], ignore_index=True, sort=True)\n",
    "\n",
    "# Here, we see the result has lost index values from df1 and df2 and retained all the information \n",
    "# filling the missing values with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can achieve the same thing using the 'append' method directly on a DataFrame.\n",
    "\n",
    "df1.append(df4, ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The concat method is not only valid for DataFrames but can also work on Series.\n",
    "\n",
    "df1 = pd.DataFrame({\"density\":[119, 206, 240, 94],\n",
    "                   \"median_age\":[42, 47, 46, 45],\n",
    "                   \"population\":[65, 60, 83, 46],\n",
    "                   \"population_change\":[0.22, -0.15, 0.32, 0.04]},\n",
    "                  index=['France', 'Italy', 'Germany', 'Spain'])\n",
    "\n",
    "s1 = pd.Series([82, 69, 93, 51], index=['France', 'Italy', 'Germany', 'Spain'], name=\"urban_population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,s1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,s1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is worth noting is that we give the series a name and then that is set to be \n",
    "# the name of the column when the two are concatenated together. \n",
    "# We could also pass in multiple series in the list and we will add a second series with world share percentage.\n",
    "\n",
    "s2 = pd.Series([0.84, 0.78, 1.07, 0.60], index=['France', 'Italy', 'Germany', 'Spain'], name=\"world_share\")\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,s1,s2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we pass in series as a list to create a DataFrame and by specifying keys we can rename the columns.\n",
    "\n",
    "s1 = pd.Series([82, 69, 93, 51], index=['France', 'Italy', 'Germany', 'Spain'], name='urban_population')\n",
    "s2 = pd.Series([0.84, 0.78, 1.07, 0.60], index=['France', 'Italy', 'Germany', 'Spain'], name='world_share')\n",
    "pd.concat([s1,s2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1,s2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1,s2], axis=1, keys=['urban population','world share'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we take our three DataFrames from before and assign them to a dictionary each with a key. \n",
    "# The dictionary is then passed into concat.\n",
    "\n",
    "df1 = pd.DataFrame({\"density\":[119, 206, 240, 94],\n",
    "                    \"median_age\":[42, 47, 46, 45],\n",
    "                    \"population\":[65, 60, 83, 46],\n",
    "                    \"population_change\":[0.22, -0.15, 0.32, 0.04]},\n",
    "                  index=['France', 'Italy', 'Germany', 'Spain'])\n",
    "\n",
    "df2 = pd.DataFrame({\"density\":[153, 464, 36, 25],\n",
    "                    \"median_age\":[38, 28, 38, 33],\n",
    "                    \"population\":[1439, 1380, 331, 212],\n",
    "                    \"population_change\":[0.39, 0.99, 0.59, 0.72]},\n",
    "                  index=['China', 'India', 'USA', 'Brazil'])\n",
    "\n",
    "df3 = pd.DataFrame({\"density\":[9, 66, 347, 103],\n",
    "                    \"median_age\":[40, 29, 48, 25],\n",
    "                    \"population\":[145, 128, 126, 102],\n",
    "                    \"population_change\":[0.04, 1.06, -0.30, 1.94]},\n",
    "                  index=['Russia', 'Mexico', 'Japan', 'Egypt'])\n",
    "\n",
    "pieces = {\"region 1\":df1, \"region 2\":df2, \"region 3\":df3}\n",
    "pd.concat(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In using a dictionary we automatically create a DataFrame with a multilevel index where\n",
    "# the first level is the key of the dictionary and the second level the index of the DataFrame.\n",
    "# We next do exactly the same but here pass in an optional keys list.\n",
    "\n",
    "pd.concat(pieces, keys=[\"region 2\",\"region 3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having looked at the concat and append methods, we now consider how pandas deals with database styles merging.\n",
    "# This is all done via the 'merge' method. \n",
    "# We will explain the specifics around each join type by example. \n",
    "# However, it is worth explaining the basics of database joins. \n",
    "# So, when we speak of database style joins, we mean the mechanism to join tables together via common values. \n",
    "# The way in which the tables will look will depend on the type of join. \n",
    "# For examples, inner, outer, right, and left joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will develop the example of country data to combine DataFrames that contain data relating to common countries\n",
    "# and now add in the data for the countries relating to the percentage world share.\n",
    "\n",
    "left = pd.DataFrame({\"density\":[119, 206, 240, 94],\n",
    "                    \"median_age\":[42, 47, 46, 45],\n",
    "                    \"population\":[65, 60, 83, 46],\n",
    "                    \"population_change\":[0.22, -0.15, 0.32, 0.04],\n",
    "                    \"country\":['France', 'Italy', 'Germany', 'Spain']})\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = pd.DataFrame({\"world_share\":[0.84, 0.78, 1.07, 0.60],\n",
    "                     \"country\":['France', 'Italy', 'Germany', 'Spain']})\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, on=\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we join two DataFrames on a common key which in this case is the country name. \n",
    "# The result is a DataFrame with only one country column where both left and right are merged.\n",
    "# In the next example, we look at the merge method with a left and right DataFrame but this time \n",
    "# will have two keys to join on which will be passed in as a list to the on argument.\n",
    "# This allows us to join on multiple values being the same.\n",
    "\n",
    "left = pd.DataFrame({\"density\":[119, 206, 240, 94],\n",
    "                    \"median_age\":[42, 47, 46, 45],\n",
    "                    \"population\":[65, 60, 83, 46],\n",
    "                    \"population_change\":[0.22, -0.15, 0.32, 0.04],\n",
    "                    \"country\":['France', 'Italy', 'Germany', 'Spain']})\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = pd.DataFrame({\"world_share\":[0.84, 0.78, 1.07, 0.60],\n",
    "                     \"population\":[65, 60, 85, 46],\n",
    "                     \"country\":['France', 'Italy', 'Germany', 'Spain']})\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, on=[\"country\",\"population\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we have joined on country and population and the resulting DataFrame is where both DataFrames share \n",
    "# the same country and population. \n",
    "# So, we lose one row of data from each DataFrame where we do not share the population and country on both.\n",
    "# Next, we run the same code with an added argument which is how equal to left.\n",
    "\n",
    "pd.merge(left, right, on=[\"country\",\"population\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of this is what is known as a left join. \n",
    "# So we retain all the information of the left DataFrame and only the elements from the right DataFrame \n",
    "# with the same keys as the left one. \n",
    "# In this case we retain all information from the left DataFrame.\n",
    "# Next we consider a right join using the same example as before:\n",
    "\n",
    "pd.merge(left, right, on=[\"country\",\"population\"], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentially, this does the same as the left join. \n",
    "# However, its now the left DataFrame that is joined onto the right one which is the reverse\n",
    "# of what we saw with the left join.\n",
    "# The next join to consider is the outer join and again for completeness, we use the previous\n",
    "# example to show how it works.\n",
    "\n",
    "pd.merge(left, right, how=\"outer\", on=[\"country\",\"population\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the outer join its a combination of both the left and right joins. \n",
    "# So, we have more rows than are in each DataFrame as the join of left and right give different results \n",
    "# so we need all of these in the outer join result.\n",
    "# The last how option we consider is the inner join.\n",
    "\n",
    "pd.merge(left, right, how=\"inner\", on=[\"country\",\"population\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This join gives only the result where we have commonality on both the left and right DataFrame. \n",
    "# This is also the default when we do not pass how argument:\n",
    "\n",
    "pd.merge(left, right, on=[\"country\",\"population\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we join two DataFrames with columns population and country but we join only on country using an outer join.\n",
    "\n",
    "left = pd.DataFrame({\"population\":[65, 60, 83, 46],\n",
    "                    \"country\":['France', 'Italy', 'Germany', 'Spain']})\n",
    "\n",
    "right = pd.DataFrame({\"population\":[65, 60, 85, 46],\n",
    "                     \"country\":['France', 'Italy', 'Germany', 'Spain']})\n",
    "\n",
    "pd.merge(left, right, how=\"outer\", on=\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we see here if the columns are the same and not used in the join the names get changed. \n",
    "# Here, we now have population_x and population_y which could be problematic if you are assuming to operate \n",
    "# on the column population. \n",
    "# This makes sense as we need a way to distinguish the two and pandas takes care of it for us.\n",
    "# Next, we do a merge using the indicator option set to True. \n",
    "# Here, we have two DataFrames with only a single column to merge on which is country and we want to do an outer join.\n",
    "\n",
    "pd.merge(left, right, on=\"country\", how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the result shows is how the join is done index by index position so this could be left, right, or both. \n",
    "# Here, we see that the join from one to the other is done on both.\n",
    "# The merge method is a pandas method to take account of two DataFrames.\n",
    "# However we can use a DataFrames join method to join one onto another.\n",
    "\n",
    "left = pd.DataFrame({\"density\":[119, 206, 240, 94],\n",
    "                    \"median_age\":[42, 47, 46, 45],\n",
    "                    \"population\":[65, 60, 83, 46],\n",
    "                    \"population_change\":[0.22, -0.15, 0.32, 0.04]},\n",
    "                   index=['France', 'Italy', 'Germany', 'Spain'])\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = pd.DataFrame({\"World_share\":[0.84, 0.78, 1.07, 0.60]},\n",
    "                    index=['France', 'Italy', 'Germany','United Kingdom'])\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.join(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we see is that the left DataFrame is retained and we join the right one where the keys in right match the keys in left. \n",
    "# Like with the merge we have the option how to join the DataFrames so we can specify that like we have seen earlier. \n",
    "# Using the same example previously we can show this.\n",
    "\n",
    "left.join(right, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using the outer join, we retain all the information from both DataFrames as we have seen when using 'merge' method. \n",
    "# And when there is no value in either one of the DataFrames, the cell will be filled in using NaN. \n",
    "# Using the same example with an inner join, following result will be shown:\n",
    "\n",
    "left.join(right, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As expected, the inner join just retains where the DataFrames have common data \n",
    "# which here is for index France, Italy, and Germany.\n",
    "\n",
    "# We can achieve the same result without using a 'how' if we pass in some different arguments to the 'merge' method. \n",
    "# These arguments are left_index and right_index here in setting them to True. \n",
    "# We are getting the same behaviour as for the join method with how set to inner.\n",
    "\n",
    "pd.merge(left, right, right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we use the argument ‘on’ with the join method when applied to the left DataFrame.\n",
    "\n",
    "left = pd.DataFrame({\"density\":[119, 206, 240, 94],\n",
    "                    \"median_age\":[42, 47, 46, 45],\n",
    "                    \"population\":[65, 60, 83, 46],\n",
    "                    \"population_change\":[0.22, -0.15, 0.32, 0.04],\n",
    "                    \"country\":['France', 'Italy', 'Germany', 'Spain']})\n",
    "\n",
    "right = pd.DataFrame({\"world_share\":[0.84, 0.78, 1.07, 0.60]},\n",
    "                    index= ['France', 'Italy', 'Germany','United Kingdom'])\n",
    "\n",
    "left.join(right, on=\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In specifying the 'on' column, which is country, we join the index of right on this column and we see that we now \n",
    "# have a DataFrame indexed by the first DataFrame. \n",
    "# This type of approach is what you may see when using databases and you want to join on the id of the column on \n",
    "# the respective value in another table. \n",
    "# This example can be extended to multiple values in the on argument, however to do this you would require multilevel \n",
    "# indexes which will be covered later in the book. \n",
    "# We can remove any NaN values by adding the how argument and setting it to inner doing an inner join as shown below.\n",
    "\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.join(right, on=\"country\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next thing we will consider is the important concept of missing data. \n",
    "# We all hope to work with perfect datasets but the reality is we generally won’t and having the ability to work \n",
    "# with missing or bad data is an important one. \n",
    "# Luckily pandas offers some great tools for dealing with this and we begin by showing how to identify where we \n",
    "# have NaN in our dataset.\n",
    "\n",
    "left = pd.DataFrame({\"density\":[119, 206, 240, 94],\n",
    "                    \"median_age\":[42, 47, 46, 45],\n",
    "                    \"population\":[65, 60, 83, 46],\n",
    "                    \"population_change\":[0.22, -0.15, 0.32, 0.04],\n",
    "                    \"country\":['France', 'Italy', 'Germany', 'Spain']})\n",
    "\n",
    "right = pd.DataFrame({\"world_share\":[0.84, 0.78, 1.07, 0.60],\n",
    "                     \"population\":[65, 60, 85, 46],\n",
    "                     \"country\":['France', 'Italy', 'Germany', 'Spain']})\n",
    "\n",
    "result = pd.merge(left, right, how=\"outer\", on=[\"country\",\"population\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(result[\"density\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"median_age\"].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we have taken the DataFrames we have seen before and created a result DataFrame using the 'merge' method \n",
    "# with how set to outer. \n",
    "# What this has done is given us a DataFrame with NaN values and we can now demonstrate how you can find where \n",
    "# these values are within your DataFrame. \n",
    "# We first consider the pandas 'isna' method on a column of the DataFrame which tests each element to see what is \n",
    "# and what isn’t NaN. \n",
    "# To achieve the same thing, we can use the 'notna' method on a column or all of our DataFrame, or we could use \n",
    "# 'isna' method which does the opposite of 'notna'. \n",
    "# This makes it very easy to determine what is and what isn’t NaN in our DataFrame.\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"density\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"density\"].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result[\"density\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the example one step further we can drop values from a column of the whole DataFrame by using the 'dropna' method. \n",
    "# For the column we only drop the one value that is NaN.\n",
    "# However, across the whole DataFrame we remove any row that has NaN in it. \n",
    "# This may not be ideal and instead we may want to remove the row where one column has NaN and we can do that by passing \n",
    "# and columns notna to the whole DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Methods\n",
    "\n",
    "# Now, in the next example, we will show some of the methods we can apply to a DataFrame.\n",
    "# Earlier we demonstrated the sum method, however pandas has lots more to offer and we will look \n",
    "# at some of the more common mathematical ones. \n",
    "# Here, we import the package seaborn and load the iris dataset that comes with it giving us the data in a DataFrame.\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"sepal_length\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following importing the package and the iris data we can access the top of the DataFrame by using head which \n",
    "# by default gives us the top five rows, we can use the tail method to get the bottom five rows. \n",
    "# We can get a defined number of rows by just passing the number into the head or tail method and if we want just \n",
    "# the columns back we can use the columns method.\n",
    "# Having imported and accessed the data we now demonstrate some methods which we can apply.\n",
    "\n",
    "iris.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.count().sepal_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"sepal_length\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply count to both the DataFrame and the column. \n",
    "# When applied to the DataFrame we return the length of each column. \n",
    "# We can also get the specific column length by either using the column name on the end of the count method or by accessing\n",
    "# the column and then applying the count method. \n",
    "# If you want the number of rows in the DataFrame as a whole you can use the len method on DataFrame.\n",
    "\n",
    "iris.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.corr()[\"petal_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.corr()[\"petal_length\"][\"sepal_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.cov()[\"sepal_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.cov()[\"sepal_length\"][\"sepal_width\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The corr method applied to the DataFrame gives us the correlation between each variable \n",
    "# and we can limit that to one columns correlation with all others by passing the column \n",
    "# name or get the correlation between two columns by passing both column names. \n",
    "# You can also see the same applies with the cov method which calculates the covariance between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we consider the cumsum method. This provides the cumulative sum as the columns ascend. \n",
    "# Now, for those columns of numeric type the value ascends as expected with the current value added \n",
    "# to the previous value and so on to create an increasing value. \n",
    "# The difference comes when we consider a character-based column. \n",
    "# The cumulative value here is just the concatenation of the values together with the results looking very strange. \n",
    "# To make things easier to read we can restrict what we show for the return of the method by specifying a list\n",
    "# of the columns to show and as you can see we can even chain the tail command on the end.\n",
    "\n",
    "iris.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.cumsum().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.cumsum()[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the 'describe' method which gives us a number of values namely the count, mean, standard deviation, \n",
    "# minimum, maximum and the 25, 50, and 75 percentiles.\n",
    "# This method only works on columns with the type to calculate the values so we not the column species is not included. \n",
    "# We can also use this on individual columns, the manner in which we have done this in the example is not to use the \n",
    "# square bracket method to accessing a column but instead the dot approach where we can use dot and the column name \n",
    "# to access the value and then chain the describe method on the end.\n",
    "\n",
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sepal_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we consider the max value. \n",
    "# Here, when applied to the entire DataFrame we get the max of every column where a maximum value can be obtained. \n",
    "# We also show that we can apply the method on a column in the same manner as we showed in the previous example.\n",
    "\n",
    "iris.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sepal_length.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sepal_width.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "\n",
    "iris.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(iris.iloc[0,0] + iris.iloc[0,1] + iris.iloc[0,2] + iris.iloc[0,3]) / 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(iris.iloc[1][0] + iris.iloc[1][1] + iris.iloc[1][2] + iris.iloc[1][3]) / 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows\n",
    "\n",
    "iris.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.mean(1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.mean(1).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next method we look at is the mean which is a common calculation that you may want to make \n",
    "# and as before we can apply it on an individual column and we have done so here using the dot syntax. \n",
    "# We then apply the mean method but now pass in a 0 or 1 referring to whether we want to apply across columns or rows. \n",
    "# There are a number of different methods that you can apply to a DataFrame and a list of some of the more useful ones is \n",
    "# given below:\n",
    "\n",
    "# ● median: returns the arithmetic median\n",
    "# ● min: returns the minimum value\n",
    "# ● max: returns the maximum value\n",
    "# ● mode: returns the most frequent number\n",
    "# ● std: returns the standard deviation\n",
    "# ● sum: returns the arithmetic sum\n",
    "# ● var: returns the variance\n",
    "\n",
    "# These are demonstrated as follows:\n",
    "\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sepal_length.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sepal_length.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sepal_length.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sepal_length.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sepal_length.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sepal_length.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sepal_length.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Data\n",
    "\n",
    "# We next consider methods we can apply across the DataFrame and how missing data is dealt with. \n",
    "# Here, we set the DataFrame up in the way we have done so far in the section and introduce some \n",
    "# NaN entries into the DataFrame.\n",
    "\n",
    "data = pd.DataFrame({\"A\":[1, 2.1, np.nan, 4.7, 5.6, 6.8],\n",
    "                    \"B\":[.25, np.nan, np.nan, 4, 12.2, 14.4]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.where(pd.notna(data), data.mean(), axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(data.mean()[\"B\":\"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(method=\"pad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(method=\"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.interpolate(method=\"barycentric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.interpolate(method=\"spline\", order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.interpolate(method=\"polynomial\", order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So interpolate has a number of methods that you can use to interpolate between the NaN’s. \n",
    "# The default, which is executed with no argument is linear and what it does is ignore the index and treats \n",
    "# the values as equally spaced and looks to linearly fill between the values. \n",
    "# The remaining methods are all taken from scipy interpolate with a brief description given below:\n",
    "\n",
    "# ● barycentric: Constructs a polynomial that passes through a given set of points.\n",
    "# ● pchip: PCHIP one-dimensional monotonic cubic interpolation\n",
    "# ● akima: Fit piecewise cubic polynomials, given vectors x and y\n",
    "# ● spline: Spline data interpolator where we can pass the order of the spline\n",
    "# ● polynomial: Polynomial data interpolator where we can pass the order of the polynomial\n",
    "\n",
    "# For more information please refer to the scipy documentation.\n",
    "# Next, we will consider interpolate on a series and show some of the optional arguments that we can pass.\n",
    "\n",
    "ser = pd.Series([np.nan, np.nan, 5, np.nan, np.nan, np.nan, 13, np.nan])\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.interpolate(limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.interpolate(limit=1, limit_direction=\"backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.interpolate(limit=1, limit_direction=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.interpolate(limit_direction=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.interpolate(limit=1, limit_direction=\"both\", limit_area=\"inside\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.interpolate(limit_direction=\"backward\", limit_area=\"outside\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.interpolate(limit_direction=\"both\", limit_area=\"outside\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially, we interpolate using the default method which is linear, and for the rest of the example we use the default method \n",
    "# and vary the optional arguments. \n",
    "# Next, we pass the limit option and set it to 1 which says we can only interpolate one past any value so we still have NaN data\n",
    "# in the Series. \n",
    "# We next keep limit set to 1 and add another argument limit direction and set it to backward. \n",
    "# What this does is only interpolate one value next to an existing value but unlike before does it going backwards. \n",
    "# We extend this in the next example by setting the limit direction to be both which interpolates both forwards and backwards \n",
    "# for one value.\n",
    "# We next remove the limit one and keep limit direction to be both and see that all values are interpolated. \n",
    "# We next introduce the limit area option which has two options (aside from the default None) these are inside and outside. \n",
    "# When set to inside NaN’s are only filled when they are surrounded by valid values and when set to outside it only fills \n",
    "# outside valid values.\n",
    "# Here, we show examples using each of these alongside limit direction and limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we introduce the 'replace' method.\n",
    "\n",
    "iris.sepal_length.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sepal_width.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.petal_length.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.petal_width.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris.replace(2.3, 2.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.petal_width.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.replace(['setosa', 'versicolor', 'virginica'],\n",
    "             ['set', 'ver', 'vir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.replace(['setosa', 'versicolor', 'virginica'],\n",
    "             ['set', 'ver', 'vir']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.replace(['setosa', 'versicolor', 'virginica'],['set', 'ver', 'vir'])[\"species\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping\n",
    "\n",
    "# Next, we introduce the concept of grouping the data via the groupby method. \n",
    "# Grouping data is a very powerful tool as we are able to create and operate on groups of data all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby = iris.groupby(\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.groupby(\"species\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above we see the groupby applied to the iris dataset where we look to group the data based on the column species. \n",
    "# This then allows us to apply methods to the groupby object and we show the results of the sum and mean method applied to this. \n",
    "# What this is doing is applying this method to all the distinct types in species by all the columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We next demonstrate how to loop over a group. \n",
    "# Here, we set the DataFrame up as seen previously but now we loop over the group and in looping over it print the name of the\n",
    "# group and what is in that group. \n",
    "# This gives us a good visualisation of what a groupby does to the data.\n",
    "\n",
    "groupby = iris.groupby(\"species\")\n",
    "for name, group in groupby:\n",
    "    print(name)\n",
    "    print(group.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we introduce the aggregate method applied to a groupby. \n",
    "# We set the data up in the same way as seen earlier and then apply the aggregate method of the groupby object and\n",
    "# inside it pass what we want to use for this aggregation. \n",
    "# In the example, we show the np.mean method which will be applied to the group.\n",
    "\n",
    "grouped = iris.groupby(\"species\")\n",
    "grouped.aggregate(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extend the previous example by introducing the as_index argument. \n",
    "# Here, we use the same DataFrame from the previous examples and groupby species with as_index set to False. \n",
    "# What this does is create a group on species but retain species in the output as its column with the value we want to group by. \n",
    "# In this case, we apply the mean to the group and so all other columns are summed within the group.\n",
    "\n",
    "iris.groupby(\"species\", as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.groupby(\"species\", as_index=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are also methods that we can apply to a groupby object which can be useful.\n",
    "\n",
    "grouped = iris.groupby(\"species\")\n",
    "grouped.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[\"sepal_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also apply different methods to the group and in this example we show multiple ways to apply the numpy methods \n",
    "# sum, mean, and std to our grouped data. \n",
    "# So we create the same DataFrame and group as in the last examples. \n",
    "# What we can then do is use the agg method with the arguments being a list of methods to be applied and what we see is\n",
    "# that each method is applied on the group of data. \n",
    "# Lastly, here we can even apply a lambda function to the groupby.\n",
    "\n",
    "grouped = iris.groupby(\"species\")\n",
    "grouped[\"sepal_length\"].aggregate([np.sum, np.mean, np.std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.aggregate({lambda x: np.std(x, ddof=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we next show is that you can get the largest and smallest values with a group by using \n",
    "# the 'nlargest' and 'nsmallest' methods. \n",
    "# Here, the integer value you pass in gives you the number of values returned. \n",
    "# What you see is that we get the largest and smallest per group.\n",
    "\n",
    "grouped = iris.groupby(\"species\")\n",
    "grouped[\"sepal_length\"].nlargest(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[\"petal_length\"].nsmallest(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our next example introduces the apply method which can be very useful. \n",
    "# Here, we set the data up in the manner seen before and groupby column species. \n",
    "# We can then use the apply method on the group to apply whatever we pass through it to the groupby. \n",
    "# It should be noted we can also use the apply method on DataFrames and Series. \n",
    "# Here we see we have applied a custom function to the groupby.\n",
    "\n",
    "grouped = iris.groupby(\"species\")\n",
    "def f(group):\n",
    "    return pd.DataFrame({\"original\": group, \"demeaned\": group - group.mean()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[\"petal_length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[\"petal_length\"].apply(f).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next example, we introduce a nice pandas method called qcut. \n",
    "# This cuts the data into equal sized buckets based on the arguments passed in. \n",
    "# Here, we apply the qcut on the data which is the column sepal length of the iris dataset by the list of \n",
    "# values 0, 0.25, 0.5, 0.75, and 1. \n",
    "# We assign the cut to the variable factor and when passed into the groupby the mean method gives the average \n",
    "# on each bucket showing what the min and max values in the buckets are.\n",
    "\n",
    "factor = pd.qcut(iris[\"sepal_length\"], [0, 0.25, 0.5, 0.75, 1])\n",
    "factor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.groupby(factor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far we have considered grouping on single columns. \n",
    "# However, we could also group on multiple columns. \n",
    "# However, the iris dataset isn’t best setup to allow us to do this so we instead load the tips dataset. \n",
    "# The tips dataset contains the following columns:\n",
    "\n",
    "# ● total_bill\n",
    "# ● tip\n",
    "# ● sex\n",
    "# ● smoker\n",
    "# ● day\n",
    "# ● time\n",
    "# ● size\n",
    "\n",
    "# Given some of the columns only have limited responses, it makes it ideal to do a group by multiple columns. \n",
    "# So next we group by sex and smoker.\n",
    "\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = tips.groupby([\"sex\",\"smoker\"])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = tips.groupby([\"sex\",\"smoker\",\"time\"])\n",
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we see that when we group by two or three variables we increase the number of values that are returned \n",
    "# by creating more combinations within the groups.\n",
    "# A similar approach to groupby is pivot table which is a common amongst spreadsheet users. \n",
    "# The concept is to take a combination of variables and group the data by it, which can seem similar to groupby. \n",
    "# The difference is you can extend upon this to create some more complicated groupings of your dataset. \n",
    "# We will demonstrate these by example.\n",
    "\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(tips, index=[\"sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(tips, index=[\"sex\",\"smoker\",\"day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(tips, index=[\"sex\",\"smoker\",\"day\"], values=[\"tip\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above code, we use the tips as the dataset in each example and set a variety of index values starting \n",
    "# at just sex and extending to sex and smoker and then with the combination of sex, smoker and day. \n",
    "# In each example, when we pivot the data by default we end up with the average of the index across all of the \n",
    "# variables where we can take an average, so only numerical variables. \n",
    "# We don’t necessarily need to show all available variables as we have seen by passing the values argument as \n",
    "# a list of columns we want to include.\n",
    "\n",
    "# As a default, when we use the pivot table command we get the average of the variables.\n",
    "# However, we can control what we get back by passing the aggfunc argument which takes a list of \n",
    "# functions we want to apply to the data. \n",
    "# Note here that we pass the numpy mean function as well as the len from the standard Python library.\n",
    "\n",
    "pd.pivot_table(tips, index=[\"sex\",\"smoker\",\"day\"], values=[\"tip\"], aggfunc=[np.mean,len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can expand on this example by adding in the margins variable which then gives us the totals \n",
    "# associated with the rows and columns.\n",
    "\n",
    "pd.pivot_table(tips, index=[\"sex\",\"smoker\"], values=[\"tip\"], columns=[\"day\"], aggfunc=[np.mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(tips, index=[\"sex\",\"smoker\"], columns=[\"day\"], values=[\"tip\"], aggfunc=[np.mean], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in Files with Pandas\n",
    "\n",
    "# The examples in the chapter have used the datasets from Seaborn and while this is useful, pandas has a lot of methods \n",
    "# to allow you to read in external files. \n",
    "# If we relate this back to earlier in the book where we read and manipulated data within Python we can see that these \n",
    "# methods are a lot easier to use. \n",
    "# They also allow us to write back to file. \n",
    "# To show how this works we will take one of the existing datasets that we have been using and write to a csv and read \n",
    "# that back in:\n",
    "\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "x = os.getcwd()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"\\\\Files\\\\tips.csv\"\n",
    "file_path = x + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we have done is use the 'to_csv' method that the tips DataFrame has and write the data into a file called ‘tips.csv’.\n",
    "# Note this will live in the directory where this is being run from or set for (like in above example), as well as the \n",
    "# file name the index argument is set to False which prevents the DataFrame index being written to the file with the \n",
    "# other columns. \n",
    "# Now, to read this back in, we use the read_csv method from pandas and this takes the csv file and creates a DataFrame\n",
    "# with the contents of this file. \n",
    "# These methods are extremely useful as we do not have to worry about the process of writing to file or reading from file. \n",
    "# Alongside 'read_csv' method, we have other read methods for different file types and here are some of the more useful ones. \n",
    "# For a complete list, consult the pandas documentation.\n",
    "\n",
    "# ● read_excel: reads in xls, xlsx, xlsm, xlsb, odf, ods and odt file types\n",
    "# ● read_json: reads a valid json string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we take the examples in previous chapters, we created the following json and excel files called \n",
    "# boston.json and boston.xlsx. \n",
    "# We can read these into DataFrames using the following code:\n",
    "\n",
    "file_name = \"\\\\Files\\\\boston.json\"\n",
    "file_path = x + file_name\n",
    "\n",
    "data = pd.read_json(file_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"\\\\Files\\\\boston.xlsx\"\n",
    "file_path = x + file_name\n",
    "\n",
    "data = pd.read_excel(file_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see these methods provide very simple ways to load data from these common formats into DataFrames. \n",
    "# There is also a read_table method which we can use for general delimited files. \n",
    "# The 'read' methods also support operations like querying databases or even reading html but that is beyond the \n",
    "# scope of this book but well worth a look.\n",
    "# The to methods of the DataFrames are pretty similar with support for many different formats and a selection given as follows:\n",
    "\n",
    "# ● to_dict\n",
    "# ● to_json\n",
    "# ● to_html\n",
    "# ● to_latex\n",
    "# ● to_string\n",
    "\n",
    "# These are all demonstrated as follows:\n",
    "\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head().to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use some of these methods to write the data directly to file in the format with some examples below:\n",
    "\n",
    "file_name = \"\\\\Files\\\\tips.json\"\n",
    "file_path = x + file_name\n",
    "\n",
    "tips.to_json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head().to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"\\\\Files\\\\tips.html\"\n",
    "file_path = x + file_name\n",
    "\n",
    "tips.to_html(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head().to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"\\\\Files\\\\tips.latex\"\n",
    "file_path = x + file_name\n",
    "\n",
    "tips.to_latex(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"\\\\Files\\\\tips.tex\"\n",
    "file_path = x + file_name\n",
    "\n",
    "tips.to_latex(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head().to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"\\\\Files\\\\tips.txt\"\n",
    "file_path = x + file_name\n",
    "\n",
    "tips.to_string(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These methods are really useful and for correctly formatted data are a very convenient way to read data into pandas \n",
    "# and also export it from pandas.\n",
    "# What we have seen in this chapter is the advanced methods of pandas and how we can do complex data analysis. \n",
    "# We have shown how pandas allows us to manipulate data as if it were in a database allows us to join, merge, group, and pivot\n",
    "# the data in a variety of ways.\n",
    "# We have also covered some of the built in methods that pandas has and shown how we can deal with missing data. \n",
    "# The examples that we have covered have been rather simple in nature but pandas is powerful enough to deal with large datasets\n",
    "# and that makes it an extremely powerful Python package. \n",
    "# It is also worth noting that pandas plays well with many other Python packages meaning a mastery of it is essential for a \n",
    "# Python programmer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
